{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 海上风电文档分类器（Ollama GPU加速版）\n",
    "\n",
    "## 功能\n",
    "1. 使用本地Ollama模型进行文档分类\n",
    "2. 两阶段分类流程（核心领域→专业相关领域）\n",
    "3. 并行批量处理Markdown文件\n",
    "4. 自动分类到时间戳标记的文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "import random\n",
    "import sys\n",
    "import concurrent.futures\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# ===================== 配置区域 =====================\n",
    "INPUT_FOLDER = \"/home/fusion/profile/汇总需要处理的md文件/\" # 输入目录\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "MODEL_NAME = \"qwen2.5:7b\"\n",
    "MAX_CHARS_FIRST = 3000  # 首次检测读取字符数\n",
    "MAX_CHARS_SECOND = 5000  # 二次检测读取字符数\n",
    "GPU_ENABLED = True  # 模型已确保GPU加速\n",
    "NUM_GPU = 2  # 使用的GPU数量（模型已配置）\n",
    "MAX_WORKERS = min(cpu_count(), 8)  # 基于CPU核心数动态调整\n",
    "TIMEOUT = 300  # 超时时间\n",
    "RETRY_LIMIT = 3  # 重试次数\n",
    "REQUEST_INTERVAL = 0.1  # 减小请求间隔\n",
    "BATCH_SIZE = 100  # 批次大小\n",
    "PARALLEL_PROCESSING = True  # 启用并行处理\n",
    "# ===================================================\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"md_classifier_gpu.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# 生成时间戳，用于输出文件夹\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# 使用时间戳创建带时间戳的输出文件夹\n",
    "GOOD_FOLDER = os.path.join(os.path.dirname(os.path.abspath(INPUT_FOLDER)), f\"good_{timestamp}\")\n",
    "TMP_FOLDER = os.path.join(os.path.dirname(os.path.abspath(INPUT_FOLDER)), f\"tmp_{timestamp}\")\n",
    "BAD_FOLDER = os.path.join(os.path.dirname(os.path.abspath(INPUT_FOLDER)), f\"bad_{timestamp}\")\n",
    "\n",
    "os.makedirs(GOOD_FOLDER, exist_ok=True)\n",
    "os.makedirs(TMP_FOLDER, exist_ok=True)\n",
    "os.makedirs(BAD_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键词定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 领域关键词定义（优化为集合，提高查找速度）\n",
    "CORE_KEYWORDS = {\n",
    "    # 中文核心关键词\n",
    "    \"海上风电\", \"风机\", \"风力发电机\", \"海缆\", \"海底电缆\",\n",
    "    \"单桩\", \"单桩基础\", \"升压站\", \"防腐涂层\", \"LCOE\",\n",
    "    \"平准化度电成本\", \"尾流效应\", \"地质勘探\", \"风电场设计\",\n",
    "    \"海上施工\", \"风机安装\", \"运维\", \"海上风电场规划\",\n",
    "    # 英文核心关键词\n",
    "    \"offshore wind\", \"wind turbine\", \"wind generator\",\n",
    "    \"submarine cable\", \"undersea cable\", \"monopile\",\n",
    "    \"monopile foundation\", \"substation\", \"anticorrosive coating\",\n",
    "    \"levelized cost of electricity\", \"wake effect\", \"geological survey\",\n",
    "    \"wind farm design\", \"offshore construction\", \"turbine installation\",\n",
    "    \"operation and maintenance\", \"offshore wind planning\"\n",
    "}\n",
    "\n",
    "RELATED_FIELDS = {\n",
    "    # 中文邻近领域\n",
    "    \"海洋工程\", \"海洋技术\", \"动力学\", \"结构工程\", \"岩土工程\",\n",
    "    \"并网技术\", \"风场\", \"风电场\", \"腐蚀防护\", \"海上气象\",\n",
    "    \"海上可再生能源\", \"可再生能源\", \"海洋能源\",\n",
    "    \"清洁能源技术\", \"船舶工程\", \"港口建设\", \"海洋环境\",\n",
    "    # 英文邻近领域\n",
    "    \"marine engineering\", \"ocean technology\", \"dynamics\",\n",
    "    \"structural engineering\", \"geotechnical engineering\",\n",
    "    \"grid connection technology\", \"wind farm\", \"corrosion protection\",\n",
    "    \"marine meteorology\", \"offshore renewable energy\",\n",
    "    \"renewable energy\", \"marine energy\", \"clean energy technology\",\n",
    "    \"naval architecture\", \"port construction\", \"marine environment\"\n",
    "}\n",
    "\n",
    "# 预编译正则表达式（优化关键词匹配）\n",
    "ALL_KEYWORDS = CORE_KEYWORDS.union(RELATED_FIELDS)\n",
    "KEYWORD_PATTERN = re.compile(r'|'.join(re.escape(kw) for kw in ALL_KEYWORDS), re.IGNORECASE)\n",
    "CORE_KEYWORD_PATTERN = re.compile(r'|'.join(re.escape(kw) for kw in CORE_KEYWORDS), re.IGNORECASE)\n",
    "RELATED_KEYWORD_PATTERN = re.compile(r'|'.join(re.escape(kw) for kw in RELATED_FIELDS), re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提示模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首次检测提示词（核心和邻近领域）\n",
    "FIRST_PROMPT_TEMPLATE = \"\"\"\n",
    "### 海上风电领域首次分类\n",
    "**核心关键词**: {core_keywords}\n",
    "**邻近领域**: {related_fields}\n",
    "\n",
    "**判断标准**:\n",
    "1. 包含任一核心关键词或邻近领域关键词 → 是\n",
    "2. 内容涉及海洋、风能、工程等海上风电相关领域 → 是\n",
    "3. 文件可能包含与海上风电间接相关的信息 → 是\n",
    "4. 完全不涉及任何技术或相关领域内容 → 否\n",
    "\n",
    "**待评估文本(前{max_chars}字符)**:\n",
    "{content}\n",
    "\n",
    "**输出**: 仅\"是\"或\"否\"\n",
    "\"\"\"\n",
    "\n",
    "# 二次检测提示词（核心和邻近领域）\n",
    "SECOND_PROMPT_TEMPLATE = \"\"\"\n",
    "### 海上风电领域二次分类\n",
    "**核心关键词**: {core_keywords}\n",
    "**邻近领域**: {related_fields}\n",
    "\n",
    "**判断标准**:\n",
    "1. 文本中包含任何海上风电相关关键词 → 是\n",
    "2. 内容涉及海洋、能源或工程领域的通用知识 → 是\n",
    "3. 文件可能包含对海上风电有价值的信息 → 是\n",
    "4. 完全不相关的领域（如纯文学、娱乐、个人日常等） → 否\n",
    "\n",
    "**待评估文本(前{max_chars}字符)**:\n",
    "{content}\n",
    "\n",
    "**输出**: 仅\"是\"或\"否\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ollama_connection():\n",
    "    \"\"\"测试Ollama连接和模型可用性\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"测试Ollama服务连接: {OLLAMA_HOST}\")\n",
    "        response = requests.get(f\"{OLLAMA_HOST}\", timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            logger.error(f\"Ollama服务连接失败: HTTP {response.status_code}\")\n",
    "            return False\n",
    "\n",
    "        logger.info(f\"测试模型可用性: {MODEL_NAME}\")\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_HOST}/api/show\",\n",
    "            json={\"name\": MODEL_NAME},\n",
    "            timeout=15\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            logger.info(f\"模型验证成功: {MODEL_NAME}\")\n",
    "            return True\n",
    "        else:\n",
    "            logger.error(f\"模型验证失败: HTTP {response.status_code}\")\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ollama连接失败: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def find_md_files(input_folder):\n",
    "    \"\"\"递归遍历文件夹，查找所有 .md 文件\"\"\"\n",
    "    md_files = []\n",
    "    for root, _, files in os.walk(input_folder):  # 使用 os.walk 遍历文件夹\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.md'):  # 只查找 .md 文件\n",
    "                md_files.append(os.path.join(root, file))\n",
    "    return md_files\n",
    "\n",
    "def check_ollama_health():\n",
    "    \"\"\"检查Ollama服务状态\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"检查Ollama服务状态: {OLLAMA_HOST}\")\n",
    "        response = requests.get(f\"{OLLAMA_HOST}/api/tags\", timeout=10)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            logger.warning(f\"获取标签列表失败: HTTP {response.status_code}，尝试直接验证模型\")\n",
    "            model_response = requests.post(\n",
    "                f\"{OLLAMA_HOST}/api/show\",\n",
    "                json={\"name\": MODEL_NAME},\n",
    "                timeout=15\n",
    "            )\n",
    "            if model_response.status_code != 200:\n",
    "                logger.error(f\"模型验证失败: HTTP {model_response.status_code}\")\n",
    "                return False\n",
    "        else:\n",
    "            try:\n",
    "                models = [model[\"name\"] for model in response.json().get(\"models\", [])]\n",
    "                logger.info(f\"Ollama服务正常，可用模型: {', '.join(models)}\")\n",
    "                if MODEL_NAME not in models:\n",
    "                    logger.error(f\"模型 '{MODEL_NAME}' 未找到\")\n",
    "                    return False\n",
    "            except json.JSONDecodeError:\n",
    "                logger.warning(\"解析模型列表失败，但模型应该可用\")\n",
    "\n",
    "        logger.info(\"Ollama服务健康检查通过\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ollama健康检查失败: {str(e)}\")\n",
    "        logger.warning(\"忽略错误，继续执行...\")\n",
    "        return True\n",
    "\n",
    "def contains_keywords(text):\n",
    "    \"\"\"关键词快速匹配（中英文不区分大小写）\"\"\"\n",
    "    try:\n",
    "        return bool(KEYWORD_PATTERN.search(text))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"关键词匹配失败: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def check_filename_relevance(filename):\n",
    "    \"\"\"文件名多语言检测（优化正则匹配）\"\"\"\n",
    "    base_name = os.path.basename(filename).lower()\n",
    "    # 核心关键词匹配\n",
    "    if CORE_KEYWORD_PATTERN.search(base_name):\n",
    "        return \"core\"  # 直接通过\n",
    "    # 相关领域匹配\n",
    "    if RELATED_KEYWORD_PATTERN.search(base_name):\n",
    "        return \"related\"  # 需要检查内容\n",
    "    return \"unrelated\"  # 文件名无关"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(content, prompt_template, max_chars, retry_count=0):\n",
    "    \"\"\"通用分类函数（优化请求参数）\"\"\"\n",
    "    if retry_count > RETRY_LIMIT:\n",
    "        logger.warning(f\"达最大重试次数，放弃\")\n",
    "        return True  # 重试失败后默认通过（更宽松）\n",
    "\n",
    "    try:\n",
    "        prompt = prompt_template.format(\n",
    "            core_keywords=\", \".join(CORE_KEYWORDS),\n",
    "            related_fields=\", \".join(RELATED_FIELDS),\n",
    "            max_chars=max_chars,\n",
    "            content=content[:max_chars]\n",
    "        )\n",
    "\n",
    "        payload = {\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"prompt\": prompt,\n",
    "            \"options\": {\n",
    "                \"num_ctx\": 16384,  # 增加上下文长度以容纳更多内容\n",
    "                \"temperature\": 0.1,\n",
    "                \"top_p\": 0.9,  # 新增参数优化生成质量\n",
    "                \"frequency_penalty\": 0.1  # 新增参数减少重复\n",
    "            },\n",
    "            \"stream\": True\n",
    "        }\n",
    "\n",
    "        time.sleep(random.uniform(0, REQUEST_INTERVAL))\n",
    "\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_HOST}/api/generate\",\n",
    "            json=payload,\n",
    "            stream=True,\n",
    "            timeout=TIMEOUT\n",
    "        )\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            logger.error(f\"API错误: HTTP {response.status_code}\")\n",
    "            return classify_text(content, prompt_template, max_chars, retry_count + 1)\n",
    "\n",
    "        response_text = \"\"\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                try:\n",
    "                    chunk = json.loads(line.decode('utf-8'))\n",
    "                    if chunk.get(\"done\", False):\n",
    "                        break\n",
    "                    if \"response\" in chunk:\n",
    "                        response_text += chunk[\"response\"]\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "        res_text = response_text.strip()\n",
    "\n",
    "        # 响应判断\n",
    "        if \"是\" in res_text or \"相关\" in res_text or \"yes\" in res_text.lower() or \"保留\" in res_text:\n",
    "            return True\n",
    "        elif \"否\" in res_text or \"无关\" in res_text or \"no\" in res_text.lower() or \"删除\" in res_text:\n",
    "            return False\n",
    "        else:\n",
    "            logger.warning(f\"异常响应: {res_text[:50]}，默认保留\")\n",
    "            # 默认认为是相关文档（更宽松）\n",
    "            return True\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"分类失败: {str(e)}\")\n",
    "        return classify_text(content, prompt_template, max_chars, retry_count + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_first_pass(file_path):\n",
    "    \"\"\"首次处理：核心和邻近领域标准\"\"\"\n",
    "    try:\n",
    "        # 跳过已处理文件\n",
    "        relative_path = os.path.relpath(file_path, INPUT_FOLDER)\n",
    "        good_path = os.path.join(GOOD_FOLDER, relative_path)\n",
    "        bad_path = os.path.join(BAD_FOLDER, relative_path)\n",
    "\n",
    "        if os.path.exists(good_path) or os.path.exists(bad_path):\n",
    "            return (\"skip\", None)\n",
    "\n",
    "        # 空文件检测\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            os.makedirs(os.path.dirname(bad_path), exist_ok=True)\n",
    "            shutil.move(file_path, bad_path)\n",
    "            return (\"empty\", None)\n",
    "\n",
    "        # 文件名检测 - 核心领域直接通过\n",
    "        filename_relevance = check_filename_relevance(file_path)\n",
    "        if filename_relevance == \"core\":\n",
    "            os.makedirs(os.path.dirname(good_path), exist_ok=True)\n",
    "            shutil.move(file_path, good_path)\n",
    "            return (\"moved\", None)\n",
    "        elif filename_relevance == \"related\":\n",
    "            # 文件名相关的文件需要检查内容\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read(MAX_CHARS_FIRST)\n",
    "\n",
    "            if not content.strip():\n",
    "                os.makedirs(os.path.dirname(bad_path), exist_ok=True)\n",
    "                shutil.move(file_path, bad_path)\n",
    "                return (\"empty\", None)\n",
    "\n",
    "            # 分类\n",
    "            is_related = classify_text(content, FIRST_PROMPT_TEMPLATE, MAX_CHARS_FIRST)\n",
    "\n",
    "            if is_related:\n",
    "                os.makedirs(os.path.dirname(good_path), exist_ok=True)\n",
    "                shutil.move(file_path, good_path)\n",
    "                return (\"moved\", None)\n",
    "            else:\n",
    "                tmp_path = os.path.join(TMP_FOLDER, relative_path)\n",
    "                os.makedirs(os.path.dirname(tmp_path), exist_ok=True)\n",
    "                shutil.move(file_path, tmp_path)\n",
    "                return (\"tmp\", tmp_path)\n",
    "        else:  # 文件名无关\n",
    "            tmp_path = os.path.join(TMP_FOLDER, relative_path)\n",
    "            os.makedirs(os.path.dirname(tmp_path), exist_ok=True)\n",
    "            shutil.move(file_path, tmp_path)\n",
    "            return (\"tmp\", tmp_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"首次处理失败 {file_path}: {str(e)}\")\n",
    "        # 出错时默认移动到临时文件夹\n",
    "        relative_path = os.path.relpath(file_path, INPUT_FOLDER)\n",
    "        tmp_path = os.path.join(TMP_FOLDER, relative_path)\n",
    "        os.makedirs(os.path.dirname(tmp_path), exist_ok=True)\n",
    "        shutil.move(file_path, tmp_path)\n",
    "        return (\"tmp\", tmp_path)\n",
    "\n",
    "def process_second_pass(file_path):\n",
    "    \"\"\"二次处理：核心和邻近领域标准\"\"\"\n",
    "    try:\n",
    "        # 读取更多内容\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read(MAX_CHARS_SECOND)\n",
    "\n",
    "        if not content.strip():\n",
    "            relative_path = os.path.relpath(file_path, TMP_FOLDER)\n",
    "            bad_path = os.path.join(BAD_FOLDER, relative_path)\n",
    "            os.makedirs(os.path.dirname(bad_path), exist_ok=True)\n",
    "            shutil.move(file_path, bad_path)\n",
    "            return False\n",
    "\n",
    "        # 二次分类\n",
    "        is_related = classify_text(content, SECOND_PROMPT_TEMPLATE, MAX_CHARS_SECOND)\n",
    "\n",
    "        if is_related:\n",
    "            relative_path = os.path.relpath(file_path, TMP_FOLDER)\n",
    "            good_path = os.path.join(GOOD_FOLDER, relative_path)\n",
    "            os.makedirs(os.path.dirname(good_path), exist_ok=True)\n",
    "            shutil.move(file_path, good_path)\n",
    "            return True\n",
    "        else:\n",
    "            # 二次分类不通过，移动到带时间戳的bad文件夹\n",
    "            relative_path = os.path.relpath(file_path, TMP_FOLDER)\n",
    "            bad_path = os.path.join(BAD_FOLDER, relative_path)\n",
    "            os.makedirs(os.path.dirname(bad_path), exist_ok=True)\n",
    "            shutil.move(file_path, bad_path)\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"二次处理失败 {file_path}: {str(e)}\")\n",
    "        # 出错时默认保留在临时文件夹\n",
    "        return False\n",
    "\n",
    "def cleanup_empty_directories(root_folder):\n",
    "    \"\"\"清理空目录（优化为多进程）\"\"\"\n",
    "    if not PARALLEL_PROCESSING:\n",
    "        for root, dirs, _ in os.walk(root_folder, topdown=False):\n",
    "            for dir_name in dirs:\n",
    "                dir_path = os.path.join(root, dir_name)\n",
    "                try:\n",
    "                    if not os.listdir(dir_path):\n",
    "                        os.rmdir(dir_path)\n",
    "                        logger.debug(f\"清理空目录: {dir_path}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"无法清理目录 {dir_path}: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # 并行清理空目录\n",
    "    dirs_to_check = []\n",
    "    for root, dirs, _ in os.walk(root_folder, topdown=False):\n",
    "        for dir_name in dirs:\n",
    "            dirs_to_check.append(os.path.join(root, dir_name))\n",
    "\n",
    "    def remove_if_empty(dir_path):\n",
    "        try:\n",
    "            if not os.listdir(dir_path):\n",
    "                os.rmdir(dir_path)\n",
    "                logger.debug(f\"清理空目录: {dir_path}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"无法清理目录 {dir_path}: {str(e)}\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        list(tqdm(executor.map(remove_if_empty, dirs_to_check),\n",
    "              total=len(dirs_to_check), desc=\"清理空目录\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主处理流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logger.info(\"===== 海上风电文档分类器（优化GPU加速版）启动 =====\")\n",
    "    logger.info(f\"输入目录: {INPUT_FOLDER}\")\n",
    "    logger.info(f\"使用模型: {MODEL_NAME} | 批次大小: {BATCH_SIZE}\")\n",
    "    logger.info(f\"GPU加速: 模型已启用 | CPU工作线程: {MAX_WORKERS}\")\n",
    "\n",
    "    # 检查Ollama服务\n",
    "    if not test_ollama_connection() or not check_ollama_health():\n",
    "        logger.error(\"服务检查失败，但继续执行...\")\n",
    "\n",
    "    # 第一阶段：处理原始文件夹中的文件\n",
    "    logger.info(\"===== 开始第一阶段：处理原始文件夹中的文件 =====\")\n",
    "    first_pass_files = find_md_files(INPUT_FOLDER)  # 递归查找所有MD文件\n",
    "    total_files = len(first_pass_files)\n",
    "    if total_files == 0:\n",
    "        logger.info(\"原始文件夹中未找到MD文件，直接进入第二阶段\")\n",
    "    else:\n",
    "        logger.info(f\"共找到 {total_files} 个MD文件，开始分批次处理...\")\n",
    "\n",
    "        # 初始化统计\n",
    "        first_stats = {\n",
    "            \"moved\": 0, \"tmp\": 0, \"empty\": 0, \"error\": 0, \"skip\": 0\n",
    "        }\n",
    "        total_batch = (total_files + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "        # 分批次处理\n",
    "        for batch_idx in range(total_batch):\n",
    "            start = batch_idx * BATCH_SIZE\n",
    "            end = min((batch_idx + 1) * BATCH_SIZE, total_files)\n",
    "            batch_files = first_pass_files[start:end]\n",
    "            batch_num = batch_idx + 1\n",
    "            logger.info(f\"\\n===== 批次 {batch_num}/{total_batch} 开始（{len(batch_files)}个文件） =====\")\n",
    "\n",
    "            # 批次内统计\n",
    "            batch_stats = {\n",
    "                \"moved\": 0, \"tmp\": 0, \"empty\": 0, \"error\": 0, \"skip\": 0\n",
    "            }\n",
    "\n",
    "            # 处理批次文件\n",
    "            if not PARALLEL_PROCESSING:\n",
    "                # 串行处理（适合调试）\n",
    "                for file in tqdm(batch_files, desc=f\"批次{batch_num}处理\"):\n",
    "                    try:\n",
    "                        result, _ = process_first_pass(file)\n",
    "                        batch_stats[result] += 1\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"处理异常: {str(e)}\")\n",
    "                        batch_stats[\"error\"] += 1\n",
    "            else:\n",
    "                # 并行处理\n",
    "                with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                    futures = {executor.submit(process_first_pass, f): f for f in batch_files}\n",
    "                    with tqdm(total=len(batch_files), desc=f\"批次{batch_num}处理\") as pbar:\n",
    "                        for future in as_completed(futures):\n",
    "                            try:\n",
    "                                result, _ = future.result()\n",
    "                                batch_stats[result] += 1\n",
    "                                pbar.update(1)\n",
    "                            except Exception as e:\n",
    "                                logger.error(f\"批次{batch_num}任务异常: {str(e)}\")\n",
    "                                batch_stats[\"error\"] += 1\n",
    "                                pbar.update(1)\n",
    "\n",
    "            # 更新全局统计\n",
    "            first_stats[\"moved\"] += batch_stats[\"moved\"]\n",
    "            first_stats[\"tmp\"] += batch_stats[\"tmp\"]\n",
    "            first_stats[\"empty\"] += batch_stats[\"empty\"]\n",
    "            first_stats[\"error\"] += batch_stats[\"error\"]\n",
    "            first_stats[\"skip\"] += batch_stats[\"skip\"]\n",
    "\n",
    "            # 批次结果\n",
    "            logger.info(f\"批次{batch_num}处理结果: \"\n",
    "                        f\"通过={batch_stats['moved']}, \"\n",
    "                        f\"待二次检测={batch_stats['tmp']}, \"\n",
    "                        f\"空文件={batch_stats['empty']}, \"\n",
    "                        f\"错误={batch_stats['error']}\")\n",
    "\n",
    "        # 第一阶段结果\n",
    "        logger.info(\"\\n\" + \"=\" * 60)\n",
    "        logger.info(\"第一阶段处理完成！\")\n",
    "        logger.info(f\"第一阶段统计:\")\n",
    "        logger.info(f\"  直接通过文件: {first_stats['moved']}（已移动到good文件夹）\")\n",
    "        logger.info(f\"  待二次检测文件: {first_stats['tmp']}（已移动到tmp文件夹）\")\n",
    "        logger.info(f\"  空文件: {first_stats['empty']}（已移动到bad文件夹）\")\n",
    "        logger.info(f\"  错误文件: {first_stats['error']}（已移动到bad文件夹）\")\n",
    "        logger.info(f\"  跳过文件: {first_stats['skip']}\")\n",
    "        logger.info(\"=\" * 60)\n",
    "\n",
    "        # 清理原始文件夹中的空目录\n",
    "        cleanup_empty_directories(INPUT_FOLDER)\n",
    "        logger.info(f\"原始文件夹空目录清理完成，当前状态: {'空' if not os.listdir(INPUT_FOLDER) else '非空'}\")\n",
    "\n",
    "    # 第二阶段：处理tmp文件夹中的文件\n",
    "    logger.info(\"\\n===== 开始第二阶段：处理tmp文件夹中的文件 =====\")\n",
    "    second_pass_files = find_md_files(TMP_FOLDER)  # 递归查找所有MD文件\n",
    "    total_second_files = len(second_pass_files)\n",
    "    if total_second_files == 0:\n",
    "        logger.info(\"tmp文件夹中未找到MD文件，处理完成\")\n",
    "    else:\n",
    "        logger.info(f\"tmp文件夹中共找到 {total_second_files} 个MD文件，开始二次检测...\")\n",
    "\n",
    "        # 初始化统计\n",
    "        second_stats = {\n",
    "            \"moved\": 0, \"rejected\": 0, \"error\": 0\n",
    "        }\n",
    "        total_second_batch = (total_second_files + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "        # 分批次处理\n",
    "        for batch_idx in range(total_second_batch):\n",
    "            start = batch_idx * BATCH_SIZE\n",
    "            end = min((batch_idx + 1) * BATCH_SIZE, total_second_files)\n",
    "            batch_files = second_pass_files[start:end]\n",
    "            batch_num = batch_idx + 1\n",
    "            logger.info(f\"\\n===== 二次检测批次 {batch_num}/{total_second_batch} 开始 =====\")\n",
    "\n",
    "            # 批次内统计\n",
    "            batch_stats = {\n",
    "                \"moved\": 0, \"rejected\": 0, \"error\": 0\n",
    "            }\n",
    "\n",
    "            # 处理批次文件\n",
    "            if not PARALLEL_PROCESSING:\n",
    "                # 串行处理\n",
    "                for file in tqdm(batch_files, desc=f\"批次{batch_num}二次检测\"):\n",
    "                    try:\n",
    "                        if process_second_pass(file):\n",
    "                            batch_stats[\"moved\"] += 1\n",
    "                        else:\n",
    "                            batch_stats[\"rejected\"] += 1\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"处理异常: {str(e)}\")\n",
    "                        batch_stats[\"error\"] += 1\n",
    "            else:\n",
    "                # 并行处理\n",
    "                with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                    futures = {executor.submit(process_second_pass, f): f for f in batch_files}\n",
    "                    with tqdm(total=len(batch_files), desc=f\"批次{batch_num}二次检测\") as pbar:\n",
    "                        for future in as_completed(futures):\n",
    "                            try:\n",
    "                                if future.result():\n",
    "                                    batch_stats[\"moved\"] += 1\n",
    "                                else:\n",
    "                                    batch_stats[\"rejected\"] += 1\n",
    "                                pbar.update(1)\n",
    "                            except Exception as e:\n",
    "                                logger.error(f\"批次{batch_num}任务异常: {str(e)}\")\n",
    "                                batch_stats[\"error\"] += 1\n",
    "                                pbar.update(1)\n",
    "\n",
    "            # 更新全局统计\n",
    "            second_stats[\"moved\"] += batch_stats[\"moved\"]\n",
    "            second_stats[\"rejected\"] += batch_stats[\"rejected\"]\n",
    "            second_stats[\"error\"] += batch_stats[\"error\"]\n",
    "\n",
    "            # 批次结果\n",
    "            logger.info(f\"二次检测批次{batch_num}结果: \"\n",
    "                        f\"通过={batch_stats['moved']}（已移动到good文件夹）, \"\n",
    "                        f\"拒绝={batch_stats['rejected']}（已移动到bad文件夹）, \"\n",
    "                        f\"错误={batch_stats['error']}\")\n",
    "\n",
    "        # 第二阶段结果\n",
    "        logger.info(\"\\n\" + \"=\" * 60)\n",
    "        logger.info(\"第二阶段处理完成！\")\n",
    "        logger.info(f\"第二阶段统计:\")\n",
    "        logger.info(f\"  二次检测通过文件: {second_stats['moved']}（已移动到good文件夹）\")\n",
    "        logger.info(f\"  最终拒绝文件: {second_stats['rejected']}（已移动到bad文件夹）\")\n",
    "        logger.info(f\"  错误文件: {second_stats['error']}（保留在tmp文件夹）\")\n",
    "        logger.info(\"=\" * 60)\n",
    "\n",
    "        # 清理tmp文件夹中的空目录\n",
    "        cleanup_empty_directories(TMP_FOLDER)\n",
    "        logger.info(f\"tmp文件夹空目录清理完成，当前状态: {'空' if not os.listdir(TMP_FOLDER) else '非空'}\")\n",
    "\n",
    "    # 最终统计\n",
    "    logger.info(\"\\n\" + \"=\" * 60)\n",
    "    logger.info(\"所有文件处理完成！\")\n",
    "    logger.info(f\"最终统计:\")\n",
    "    logger.info(f\"  通过文件总数: {first_stats.get('moved', 0) + second_stats.get('moved', 0)}\")\n",
    "    logger.info(f\"  拒绝文件: {first_stats.get('empty', 0) + second_stats.get('rejected', 0)}\")\n",
    "    logger.info(f\"  临时保留文件: {first_stats.get('tmp', 0) + second_stats.get('error', 0)}\")\n",
    "    logger.info(f\"  原始文件夹状态: {'空' if not os.listdir(INPUT_FOLDER) else '非空'}\")\n",
    "    logger.info(f\"  通过文件存放于: {GOOD_FOLDER}\")\n",
    "    logger.info(f\"  拒绝文件存放于: {BAD_FOLDER}\")\n",
    "    logger.info(f\"  临时保留文件存放于: {TMP_FOLDER}\")\n",
    "    logger.info(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 执行主程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"程序被用户中断\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"程序异常终止: {str(e)}\")\n",
    "        sys.exit(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
